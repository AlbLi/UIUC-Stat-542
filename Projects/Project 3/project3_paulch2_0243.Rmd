---
title: "(PSL) Project 3"
author: "Paul Holaway (paulch2), Albert Li (xiangl9), & Matt Schroeder (mas5)"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(glmnet)
library(text2vec)
library(tidyverse)
library(tm)
library(slam)
library(stopwords)
library(stringr)

#library('SnowballC')
#library('naivebayes')
#library('kernlab')
#library('Tensorflow')
#library('Fasttext')
#library('glmnet')
```


# Creating Splits

```{r}
# reading in all data with a header
data <- read.table("alldata.tsv", stringsAsFactors = FALSE,
                  header = TRUE)
# reading in the training/testing splits (25,000 each)
testIDs <- read.csv("project3_splits.csv", header = TRUE)

# creating files for each of the five splits and writing training/testing data to corresponding TSV files
for(j in 1:5){
  dir.create(paste("split_", j, sep=""))
  train <- data[-testIDs[,j], c("id", "sentiment", "review") ]
  test <- data[testIDs[,j], c("id", "review")]
  test.y <- data[testIDs[,j], c("id", "sentiment", "score")]
  
  tmp_file_name <- paste("split_", j, "/", "train.tsv", sep="")
  write.table(train, file=tmp_file_name, 
              quote=TRUE, 
              row.names = FALSE,
              sep='\t')
  tmp_file_name <- paste("split_", j, "/", "test.tsv", sep="")
  write.table(test, file=tmp_file_name, 
              quote=TRUE, 
              row.names = FALSE,
              sep='\t')
  tmp_file_name <- paste("split_", j, "/", "test_y.tsv", sep="")
  write.table(test.y, file=tmp_file_name, 
            quote=TRUE, 
            row.names = FALSE,
            sep='\t')
}
```

# What We Have Tried (I)

```{r}
# working on the first split
j = 1
setwd(paste("split_", j, sep=""))
train = read.table("train.tsv",
                   stringsAsFactors = FALSE,
                   header = TRUE)
# removing specials characters from each review in train
train$review = gsub('<.*?>', ' ', train$review)
```


All of the functions below that you don't see on a regular basis are from the `text2vec` library.

As far as I can tell (subject to change) this reduces the vocabulary. It is unclear how this vocabulary reduced should be stored and how it can be used to build a predictive model with a AUC > .96.


- DTM: document term matrix
- stop words: filtered out because they are insignificant in NLP data

```{r}
set.seed(0243)
# stop words that shouldn't be considered in the analysis
stop_words = c("i", "me", "my", "myself", 
               "we", "our", "ours", "ourselves", 
               "you", "your", "yours", 
               "their", "they", "his", "her", 
               "she", "he", "a", "an", "and",
               "is", "was", "are", "were", 
               "him", "himself", "has", "have", 
               "it", "its", "the", "us")
it_train = itoken(train$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)
tmp.vocab = create_vocabulary(it_train, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)
dtm_train  = create_dtm(it_train, vocab_vectorizer(tmp.vocab))

# test df
#it_test = word_tokenizer(tolower(test$review))

#it_test = itoken(it_test, ids = test$id,
                 # turn off progressbar because it won't look nice in rmd
#                progressbar = FALSE)

it_test = itoken(test$review,
                  preprocessor = tolower, 
                  tokenizer = word_tokenizer)

tmp.vocab = create_vocabulary(it_test, 
                              stopwords = stop_words, 
                              ngram = c(1L,4L))
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10,
                             doc_proportion_max = 0.5,
                             doc_proportion_min = 0.001)

dtm_test = create_dtm(it_test, vocab_vectorizer(tmp.vocab))

# only keep columns in both DFs
cols_to_keep <- intersect(colnames(dtm_test),colnames(dtm_train))
dtm_test <- dtm_test[,cols_to_keep, drop=FALSE]
dtm_train <- dtm_train[,cols_to_keep, drop=FALSE]
dim(dtm_test)
dim(dtm_train)

#Reducing vocab with Lasso

tmpfit = cv.glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial',
                type.measure = 'auc',
                nfolds = 5)

# I changed this from glmnet to cv.glmnet and added nfolds and type.measure
#tmpfit = cv.glmnet(x = dtm_train, 
#                y = train$sentiment, 
#                alpha = 1,
#                family='binomial',
#                nfolds = 5,
#                type.measure = 'auc')
#tmpfit$df
# given code ends

# NEW prediction code
preds = predict(tmpfit, dtm_test, type = 'response')[,1]
glmnet:::auc(test.y$sentiment, preds)
```

```{r}
plot(tmpfit)
print(paste("max AUC =", round(max(tmpfit$cvm), 4)))
```
```{r}
dim(dtm_test)
dim(dtm_train)
```

In theory the code below reduces the vocab size and runs the model again.

```{r}
# determining how many of coefs are non-zero
count = sum(coef(tmpfit) !=0)
count
head(coef(tmpfit))
```
```{r}
head(colnames(dtm_train))
```

```{r}
typeof(colnames(dtm_train))
```

```{r}
# writing a for loop that adds non-zero colnames to a list
nz = rep(0,length(coef(tmpfit)))
count = 0
for (i in 1:length(coef(tmpfit))) {
  if (coef(tmpfit)[i] != 0) {
    nz[i] = 1
  }
}
sum(nz)
```

```{r}
# adding non-zero colnames to v
v = c()
for (i in 1:length(coef(tmpfit))) {
  if (nz[i] == 1) {
    v <- append(v,colnames(dtm_train)[i])
  }
}
length(v)
head(v)

```

```{r}
# new df is intersection of non-zero coefs and tmp-train + tmp-test
cols_to_keep2 <- intersect(v,colnames(dtm_train))
dtm_test <- dtm_test[,cols_to_keep2, drop=FALSE]
dtm_train <- dtm_train[,cols_to_keep2, drop=FALSE]
dim(dtm_test)
dim(dtm_train)
# re-run analysis
tmpfit2 = cv.glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial',
                type.measure = 'auc',
                nfolds = 5)

# I changed this from glmnet to cv.glmnet and added nfolds and type.measure
#tmpfit = cv.glmnet(x = dtm_train, 
#                y = train$sentiment, 
#                alpha = 1,
#                family='binomial',
#                nfolds = 5,
#                type.measure = 'auc')
#tmpfit$df
# given code ends

# NEW prediction code
preds = predict(tmpfit2, dtm_test, type = 'response')[,1]
glmnet:::auc(test.y$sentiment, preds)
```

```{r}
plot(tmpfit2)
print(paste("max AUC =", round(max(tmpfit2$cvm), 4)))
```

I don't believe the code from here to # What We Have Tried (II) has much utility but keep it now just in case.


```{r}
# given code restarts
#Storing words such that # < 2,000; Occurs for beta in 78th estimate
myvocab = colnames(dtm_train)[which(tmpfit$beta[, 78] != 0)]
#Model Making

it_train = itoken(train$review,
                    preprocessor = tolower, 
                    tokenizer = word_tokenizer)
vectorizer = vocab_vectorizer(create_vocabulary(myvocab, 
                                                  ngram = c(1L, 2L)))
dtm_train = create_dtm(it_train, vectorizer)

it_test = word_tokenizer(tolower(test$review))

it_test = itoken(it_test, ids = test$id,
                 # turn off progressbar because it won't look nice in rmd
                 progressbar = FALSE)

vectorizer = vocab_vectorizer(create_vocabulary(myvocab, 
                                                  ngram = c(1L, 2L)))
dtm_test = create_dtm(it_test, vectorizer)

cols_to_keep <- intersect(colnames(dtm_test),colnames(dtm_train))
dtm_test <- dtm_test[,cols_to_keep, drop=FALSE]
dtm_train <- dtm_train[,cols_to_keep, drop=FALSE]

# dim of matricies
dim(dtm_test)
dim(dtm_train)

tmpfit2 = cv.glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial',
                nfolds = 5,
                type.measure = 'auc')
#tmpfit$df
# given code ends

# NEW prediction code
preds2 = predict(tmpfit2, dtm_test, type = 'response')[,1]
glmnet:::auc(test.y$sentiment, preds2)
```






```{r}
# OLD prediction code
it_test = word_tokenizer(tolower(test$review))
it_test = itoken(it_test, ids = test$id,
                 # turn off progressbar because it won't look nice in rmd
                 progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(tmpfit, dtm_test, type = 'response')[,1]
glmnet:::auc(test.y$sentiment, preds)
```









```{r}
dim(dtm_train)
#dtm_train[,dtm_train[1,] != 0]
dtm_train[1,dtm_train[1,] != 0]
```


```{r}
#head(myvocab,100)
typeof(myvocab[29])
myvocab[29]
# replacing the _ with a space
gsub( '_', ' ',myvocab[29])
```

## Messing Around With the Output From WWHT (I)
```{r}
# checking if the string is present in the text review
grepl(myvocab[29],'but_neither he and her')
```

```{r}
train[1,3]
```


```{r}
# removing words in train and test that don't appear in myvocab
train[1,3]
for (i in 1:1) {
  temp = length(t)
}
```



# What We Have Tried (II)

```{r}
#Compute mean and variance for each column of dtm_train
v.size = dim(dtm_train)[2]
ytrain = train$sentiment

summ = matrix(0, nrow=v.size, ncol=4)
summ[,1] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), mean)
summ[,2] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==1, ]), var)
summ[,3] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), mean)
summ[,4] = colapply_simple_triplet_matrix(
  as.simple_triplet_matrix(dtm_train[ytrain==0, ]), var)

n1 = sum(ytrain); 
n = length(ytrain)
n0 = n - n1

myp = (summ[,1] - summ[,3])/
  sqrt(summ[,2]/n1 + summ[,4]/n0)
#Ordering words by magnitude
words = colnames(dtm_train)
id = order(abs(myp), decreasing=TRUE)[1:2000]
pos.list = words[id[myp[id]>0]]
neg.list = words[id[myp[id]<0]]
#Reducing vocab with Lasso again
tmpfit = glmnet(x = dtm_train, 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
tmpfit$df
#Storing words such that # < 1,000; Occurs for beta in 35th estimate
myvocab = colnames(dtm_train)[which(tmpfit$beta[, 35] != 0)]
#Model Making
 it_train = itoken(train$review,
                    preprocessor = tolower, 
                    tokenizer = word_tokenizer)
 vectorizer = vocab_vectorizer(create_vocabulary(myvocab, 
                                                  ngram = c(1L, 2L)))
 dtm_train = create_dtm(it_train, vectorizer)
```

```{r}

```


```{r}
pos.list
```
```{r}
id
```


```{r}
tmpfit2 = glmnet(x = dtm_train[id,], 
                y = train$sentiment, 
                alpha = 1,
                family='binomial')
#summary(tmpfit2)
```

# Model

```{r}
#####################################
# Load libraries
# Load your vocabulary and training data
#####################################
myvocab <- scan(file = "myvocab.txt", what = character())
train <- read.table("train.tsv", stringsAsFactors = FALSE,
                   header = TRUE)

#####################################
# Train a binary classification model
#####################################


#####################################
# Load test data, and 
# Compute prediction
#####################################
test <- read.table("test.tsv", stringsAsFactors = FALSE,
                    header = TRUE)


#####################################
# Store your prediction for test data in a data frame
# "output": col 1 is test$id
#           col 2 is the predicted probs
#####################################
write.table(output, file = "mysubmission.txt", 
            row.names = FALSE, sep='\t')
```

# Evaluation Code

```{r}
library(pROC)
source("mymain.R")
# move "test_y.tsv" to this directory
test.y <- read.table("test_y.tsv", header = TRUE)
pred <- read.table("mysubmission.txt", header = TRUE)
pred <- merge(pred, test.y, by="id")
roc_obj <- roc(pred$sentiment, pred$prob)
pROC::auc(roc_obj)
```

